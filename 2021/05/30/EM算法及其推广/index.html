<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>《统计机器学习》读书笔记 · EM算法及其推广 | xyy爱吃番茄</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="《统计学习方法》EM 算法的部分读书笔记">
<meta property="og:type" content="article">
<meta property="og:title" content="《统计机器学习》读书笔记 · EM算法及其推广">
<meta property="og:url" content="http://www.xuyuyan.cn/2021/05/30/EM%E7%AE%97%E6%B3%95%E5%8F%8A%E5%85%B6%E6%8E%A8%E5%B9%BF/index.html">
<meta property="og:site_name" content="xyy爱吃番茄">
<meta property="og:description" content="《统计学习方法》EM 算法的部分读书笔记">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://qiniu.xuyuyan.cn/image-20210530150836548.png">
<meta property="og:image" content="https://qiniu.xuyuyan.cn/image-20210530151122965.png">
<meta property="og:image" content="https://qiniu.xuyuyan.cn/image-20210530155612639.png">
<meta property="og:image" content="https://qiniu.xuyuyan.cn/image-20210530164519762.png">
<meta property="og:image" content="https://qiniu.xuyuyan.cn/image-20210530191432219.png">
<meta property="og:image" content="https://qiniu.xuyuyan.cn/image-20210530191902761.png">
<meta property="og:image" content="https://qiniu.xuyuyan.cn/image-20210530200004997.png">
<meta property="og:image" content="https://qiniu.xuyuyan.cn/image-20210530200754651.png">
<meta property="og:image" content="https://qiniu.xuyuyan.cn/image-20210530202113984.png">
<meta property="og:image" content="https://qiniu.xuyuyan.cn/image-20210530203611888.png">
<meta property="og:image" content="https://qiniu.xuyuyan.cn/image-20210530204919283.png">
<meta property="og:image" content="https://qiniu.xuyuyan.cn/image-20210530205311712.png">
<meta property="og:image" content="https://qiniu.xuyuyan.cn/image-20210530205311712.png">
<meta property="article:published_time" content="2021-05-30T13:40:43.000Z">
<meta property="article:modified_time" content="2021-05-30T13:50:28.000Z">
<meta property="article:author" content="xyy">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="统计学习方法">
<meta property="article:tag" content="EM 算法">
<meta property="article:tag" content="高斯分布">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://qiniu.xuyuyan.cn/image-20210530150836548.png">
  
    <link rel="alternate" href="/atom.xml" title="xyy爱吃番茄" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">xyy爱吃番茄</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS 订阅"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="搜索"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://www.xuyuyan.cn"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="《统计机器学习》读书笔记 · EM算法及其推广-EM算法及其推广" class="h-entry article article-type-《统计机器学习》读书笔记 · EM算法及其推广" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/05/30/EM%E7%AE%97%E6%B3%95%E5%8F%8A%E5%85%B6%E6%8E%A8%E5%B9%BF/" class="article-date">
  <time class="dt-published" datetime="2021-05-30T13:40:43.000Z" itemprop="datePublished">2021-05-30</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">《统计学习方法》读书笔记</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      《统计机器学习》读书笔记 · EM算法及其推广
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>《统计学习方法》EM 算法的部分读书笔记</p>
<span id="more"></span>

<h3 id="1-引入-·-三硬币模型"><a href="#1-引入-·-三硬币模型" class="headerlink" title="1. 引入 · 三硬币模型"></a>1. 引入 · 三硬币模型</h3><p>​    假设有三枚硬币，分别记作A、B、C。这三枚硬币正面朝上的概率分别为 $\pi$、$p$ 和 $q$。进行如下掷硬币实验：先掷硬币 A，如果 A 为正面则下一个掷 B，反面掷 C；以第二枚硬币的结果为最终结果。进行独立重复实验。</p>
<p>​    我们将独立重复实验得到的结果记作 “<font style="background: #F9EDA6">观测结果</font>”；将每次硬币 A 的结果记为 “<font style="background: #F9EDA6">不可观测数据（隐变量）</font>”。假设我们只能知道观测结果，如何去估计隐变量？这就是 EM 算法解决的问题。（<font style="background: #FBD4D0">用于含有隐变量的概率模型参数的极大似然估计</font>）</p>
<br>

<h3 id="2-EM-算法的导出"><a href="#2-EM-算法的导出" class="headerlink" title="2. EM 算法的导出"></a>2. EM 算法的导出</h3><blockquote>
<p>算法：<font style="background: #d4e9d6">（EM算法）</font></p>
<p>输入：模型参数 $\theta$</p>
<ol>
<li><p>选择参数的初值 $\theta^{(0)}$，开始迭代。</p>
</li>
<li><p>E 步（求期望）：记 $\theta^{(i)}$ 为第 i 次迭代参数 $\theta$ 的估计值，在第 i+1 次迭代的 E 步，计算：</p>
<p>$Q(\theta,\theta^{(i)})=E_Z[logP(Y,Z|\theta)|Y, \theta^{(i)}]$</p>
<p>$=\sum_ZlogP(Y,Z|\theta)P(Z|Y,\theta^{(i)})$</p>
<p>此处，$P(Z|Y,\theta^{(i)})$ 是给定观测数据 Y 和当前的参数估计 $\theta^{(i)}$ 下隐变量数据 Z 的条件概率分布。</p>
</li>
<li><p>M 步（求极大）：求使 $Q(\theta,\theta^{(i)})$ 极大化的 $\theta$，确定第 i+1 次迭代的参数的估计值 $\theta^{(i+1)}$ </p>
<p>$\theta^{(i+1)}=arg\ max_{\theta}Q(\theta, \theta^{(i)})$</p>
</li>
<li><p>重复第二步和第三步，直到收敛。</p>
</li>
</ol>
</blockquote>
<p>接下来对 EM 算法进行推导：</p>
<p>我们的目标是<font style="background: #F9EDA6">极大化观测数据 Y 关于参数 $\theta$ 的对数似然函数</font>，也就是 $L(\theta)=log P(Y|\theta)$</p>
<img src="https://qiniu.xuyuyan.cn/image-20210530150836548.png" alt="image-20210530150836548" style="zoom:33%;" /> 

<p>将上面的不等式进行整理：</p>
 <img src="https://qiniu.xuyuyan.cn/image-20210530151122965.png" alt="image-20210530151122965" style="zoom:33%;" />

<p>为了使 $L(\theta)$ 有尽可能大的增长，选择 $\theta^{(i+1)}$ 使 $B(\theta, \theta^{(i)})$ 达到极大：$\theta^{(i+1)}=arg\ max_{\theta}B(\theta, \theta^{(i)})$</p>
<p>对这个表达式进行化简：</p>
 <img src="https://qiniu.xuyuyan.cn/image-20210530155612639.png" alt="image-20210530155612639" style="zoom:33%;" />

<p>EM 算法就是<font style="background: #F9EDA6">通过不断对 Q 函数（下界）求极大，逼近求解对数似然函数极大化</font>。</p>
<p>将上述算法步骤进行总结：</p>
 <img src="https://qiniu.xuyuyan.cn/image-20210530164519762.png" alt="image-20210530164519762" style="zoom:33%;" />

<br>

<h3 id="3-EM-算法的收敛性"><a href="#3-EM-算法的收敛性" class="headerlink" title="3. EM 算法的收敛性"></a>3. EM 算法的收敛性</h3><p>​    在第二章中，我们并未对以下两个问题进行分析：</p>
<ol>
<li><p><font style="background: #FBD4D0">EM 算法的估计序列是否收敛？</font></p>
</li>
<li><p><font style="background: #FBD4D0">如果收敛，是否收敛到全局最大值或者局部最大值？</font></p>
<p>我们将通过以下两个定理进行介绍。</p>
</li>
</ol>
<h4 id="3-1-递增性"><a href="#3-1-递增性" class="headerlink" title="3.1 递增性"></a>3.1 递增性</h4><blockquote>
<p>定理：设 $P(Y|\theta)$ 为观测数据的似然函数，$\theta^{(i)}$ 为 EM 算法得到的参数估计序列，$P(Y|\theta^{(i)})$ 为对应的似然函数序列，则 <font style="background: #d4e9d6">$P(Y|\theta^{(i)})$ 是单调递增的</font>。</p>
</blockquote>
<p>接下来我们进行证明：</p>
 <img src="https://qiniu.xuyuyan.cn/image-20210530191432219.png" alt="image-20210530191432219" style="zoom:33%;" />

<p>若要证明 $P(Y|\theta)$ 是单调递增的，只需证明上式大于等于 0。</p>
 <img src="https://qiniu.xuyuyan.cn/image-20210530191902761.png" alt="image-20210530191902761" style="zoom:33%;" />

<p>至此， $P(Y|\theta)$ 是单调递增得证。</p>
<h4 id="3-2-收敛到极大值"><a href="#3-2-收敛到极大值" class="headerlink" title="3.2 收敛到极大值"></a>3.2 收敛到极大值</h4><blockquote>
<p>定理：设 $L(\theta)=logP(Y|\theta)$ 为观测数据的对数似然函数，$\theta^{(i)}$ 为 EM 算法得到的参数估计序列，$L(\theta^{(i)})$ 为对应的对数似然函数序列。</p>
<ol>
<li><font style="background: #d4e9d6">如果 $P(Y|\theta)$ 有上界，则 $L(\theta^{(i)})=logP(Y|\theta^{(i)})$ 收敛到某一值 $L^*$</font></li>
</ol>
</blockquote>
<p>由 $L(\theta)=logP(Y|\theta^{(i)})$ 的单调性及 $P(Y|\theta)$ 的有界性可以立即得到。</p>
<br>

<h3 id="4-EM-算法在高斯混合模型学习中的应用"><a href="#4-EM-算法在高斯混合模型学习中的应用" class="headerlink" title="4. EM 算法在高斯混合模型学习中的应用"></a>4. EM 算法在高斯混合模型学习中的应用</h3><h4 id="4-1-高斯分布"><a href="#4-1-高斯分布" class="headerlink" title="4.1 高斯分布"></a>4.1 高斯分布</h4><p>先简单介绍一下<font style="background: #d4e9d6">高斯分布</font></p>
<p>$\phi(y|\theta)=\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$</p>
<p>其中，$\mu$ 决定分布的位置；$\sigma^2$ 为方差，决定分布的幅度。</p>
<h4 id="4-2-高斯混合模型"><a href="#4-2-高斯混合模型" class="headerlink" title="4.2 高斯混合模型"></a>4.2 高斯混合模型</h4><p>高斯混合模型可以理解为<font style="background: #F9EDA6">由多个高斯分布叠加得到</font>，具有如下概率分布：</p>
<p>$P(y|\theta)=\sum_{k=1}^K\alpha_k\phi(y|\theta_k)$</p>
<p>其中 K 为 K 个叠加的高斯模型，$\alpha_k$ 为每个高斯模型的权重，$\phi(y|\theta_k)$ 为每个高斯模型。</p>
<h4 id="4-3-高斯混合模型参数估计的-EM-算法"><a href="#4-3-高斯混合模型参数估计的-EM-算法" class="headerlink" title="4.3 高斯混合模型参数估计的 EM 算法"></a>4.3 高斯混合模型参数估计的 EM 算法</h4><ul>
<li><p><font style="background: #CCE0F1">步骤一：明确隐变量 </font></p>
<p>设想观测数据 $y_j$ 的产生方式为：首先依据概率 $\alpha_k$ 选择 k 个高斯分布模型 $\phi(y|\theta_k)$；然后依第 k 个分模型的概率分布 $\phi(y|\theta_k)$ 生成观测数据 $y_j$。</p>
<p>其中，反映<font style="background: #F9EDA6">观测数据 $y_j$ 来自第 k 个分模型的数据是未知的，记作隐变量 $\gamma_{jk}$</font>。</p>
<p>$\gamma_{jk}=\begin{cases} 1 &amp;\text{第 j 个观测来自第 k 个模型} \\ 0 &amp;\text{否则} \end{cases}$ </p>
<p>通过观测数据 $y_j$ 和未观测数据 $\gamma_{jk}$，得到完全数据 $(y_j,\gamma_{j1},\gamma_{j2},…,\gamma_{jK})$</p>
<p>因此可以写出完全数据的<font style="background: #F9EDA6">似然函数</font>：</p>
 <img src="https://qiniu.xuyuyan.cn/image-20210530200004997.png" alt="image-20210530200004997" style="zoom:33%;" />

<p>令 $n_k=\sum_{j=1}^N\gamma_{jk}$ 可得：</p>
 <img src="https://qiniu.xuyuyan.cn/image-20210530200754651.png" alt="image-20210530200754651" style="zoom:33%;" />

<p>则完全数据的<font style="background: #F9EDA6">对数似然函数</font>为：</p>
 <img src="https://qiniu.xuyuyan.cn/image-20210530202113984.png" alt="image-20210530202113984" style="zoom:33%;" /></li>
<li><p><font style="background: #CCE0F1">步骤二：（E）确定 Q 函数</font></p>
 <img src="https://qiniu.xuyuyan.cn/image-20210530203611888.png" alt="image-20210530203611888" style="zoom:33%;" />

<p>此处需要<font style="background: #F9EDA6">计算 $E(\gamma_{jk},\theta)$</font>，记作 $\hat{\gamma_{jk}}$</p>
 <img src="https://qiniu.xuyuyan.cn/image-20210530204919283.png" alt="image-20210530204919283" style="zoom:33%;" /></li>
<li><p><font style="background: #CCE0F1">步骤三：（M）迭代求 Q 函数对 $\theta$ 的最大值（即求新一轮迭代的参数）</font></p>
<p>求 Q 函数的极大值，仅需<font style="background: #F9EDA6">对各参数（$\mu_k$，$\sigma_k^2$）求偏导并令其为 0</font>（拉格朗日参数法）</p>
<p>求 $\hat{\alpha_k}$ 是在 $\sum_{k=1}^K\alpha_k=1$ 下求偏导数并令其为 0（拉格朗日参数法）</p>
<p>可以得到如下结果</p>
 <img src="https://qiniu.xuyuyan.cn/image-20210530205311712.png" alt="image-20210530205311712" style="zoom:60%;" /></li>
</ul>
<p><font style="background: #F9EDA6">重复步骤二和步骤三，直到对数似然函数值不再有明显的变化。</font></p>
<h4 id="4-5-高斯混合模型参数估计-EM-算法总结"><a href="#4-5-高斯混合模型参数估计-EM-算法总结" class="headerlink" title="4.5 高斯混合模型参数估计 EM 算法总结"></a>4.5 高斯混合模型参数估计 EM 算法总结</h4><blockquote>
<p>输入：观测数据 $y_1$，$y_2$，…，$y_N$；高斯混合模型</p>
<p>输出：高斯混合模型参数</p>
<ol>
<li><p>取参数的初始值开始迭代</p>
</li>
<li><p>E 步：依据当前参数模型，计算分模型 k 对观测数据 $y_j$ 的响应度</p>
<p>$\hat{\gamma_{jk}}=\frac{\alpha_k\phi(y_j|\theta_k)}{\sum_{k=1}^K\alpha_k\phi(y_j|\theta_k)}$</p>
</li>
<li><p>M 步：计算新一轮迭代的模型参数</p>
 <img src="https://qiniu.xuyuyan.cn/image-20210530205311712.png" alt="image-20210530205311712" style="zoom:60%;" /></li>
<li><p>重复步骤（2）和（3），直至收敛</p>
</li>
</ol>
</blockquote>
<br>

<h3 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h3><p>本章主要介绍了 EM 算法，其本质就是<font style="background: #FBD4D0">通过不断迭代参数逼近似然函数的极大值</font></p>
<p>在第二章我们<font style="background: #FBD4D0">介绍了 EM 算法</font>，EM 算法顾名思义分为两步：</p>
<ol>
<li><p>E 步，求 Q 函数的期望（对数似然函数的期望）。</p>
</li>
<li><p>M 步，求使 Q 函数极大化的参数，确定下一次迭代的参数的估计值</p>
<p>$\theta^{(i+1)}=arg\ max_{\theta}Q(\theta,\theta^{(i)})$</p>
</li>
</ol>
<p>在第三章我们<font style="background: #FBD4D0">对 EM 算法进行了推导</font>：</p>
<ol>
<li>我们的目的是极大化对数似然函数 $L(\theta)$，因此找到对数似然函数的下限 $B(\theta,\theta^{(i)})$ ,将待解决的问题转换为 “寻找合适的参数使 $B(\theta,\theta^{(i)})$ 极大”。</li>
<li>通过对 $B(\theta,\theta^{(i)})$ 进行化简，我们可以得到 Q 函数，因此问题转化为 “寻找合适的参数使 Q 函数极大”，并且极大值为下一次迭代的参数。</li>
<li>使用新的参数重新计算 Q 函数，并继续求极大，直至收敛。</li>
</ol>
<p>在第四章我们<font style="background: #FBD4D0">证明了 </font>EM 算法得到的似然函数关于 $\theta^{(i)}$ 单调递增且有界。因此<font style="background: #FBD4D0">似然函数 $L(\theta)$ 收敛有上界</font>，因此 EM 算法在理论上可行。</p>
<p>在第五章我们<font style="background: #FBD4D0">介绍了 EM 算法在解决高斯混合模型中的应用</font>，主要分为三步：</p>
<ol>
<li><p>取参数初始值开始迭代</p>
</li>
<li><p>E 步：计算对数似然函数的期望</p>
<p>特别地，需要计算分模型 k 对观测数据 $y_k$ 的响应度（当前模型参属下第 j 个观测数据来自第 k 个分模型的概率）</p>
</li>
<li><p>M 步：计算新一轮的模型参数</p>
</li>
<li><p>重复第二步和第三步，直至收敛</p>
</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://www.xuyuyan.cn/2021/05/30/EM%E7%AE%97%E6%B3%95%E5%8F%8A%E5%85%B6%E6%8E%A8%E5%B9%BF/" data-id="ckwknngo40006fcuf1cgs0b5j" data-title="《统计机器学习》读书笔记 · EM算法及其推广" class="article-share-link">分享</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/EM-%E7%AE%97%E6%B3%95/" rel="tag">EM 算法</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" rel="tag">统计学习方法</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83/" rel="tag">高斯分布</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2021/06/24/%E6%AF%95%E4%B8%9A%E6%97%85%E8%A1%8C2021/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">前一篇</strong>
      <div class="article-nav-title">
        
          毕业旅行2021
        
      </div>
    </a>
  
  
    <a href="/2021/05/21/%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">后一篇</strong>
      <div class="article-nav-title">《统计学习方法》读书笔记 · 提升方法</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">分类</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">《统计学习方法》读书笔记</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%B8%AA%E4%BA%BA%E7%A2%8E%E7%A2%8E%E5%BF%B5/">个人碎碎念</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/">操作系统</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6/">离散数学</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%AE%97%E6%B3%95/">算法</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/">计算机网络</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/">设计模式</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95/">软件测试</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E9%9D%A2%E7%BB%8F/">面经</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AdaBoost/" rel="tag">AdaBoost</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DFS/" rel="tag">DFS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/EM-%E7%AE%97%E6%B3%95/" rel="tag">EM 算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/UI%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95/" rel="tag">UI自动化测试</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/k-%E8%BF%91%E9%82%BB/" rel="tag">k 近邻</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kd-%E6%A0%91/" rel="tag">kd 树</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F/" rel="tag">代理模式</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%86%B3%E7%AD%96%E6%A0%91/" rel="tag">决策树</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%88%86%E6%B2%BB%E7%AE%97%E6%B3%95/" rel="tag">分治算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%89%8D%E5%90%91%E5%88%86%E6%AD%A5%E6%96%B9%E6%B3%95/" rel="tag">前向分步方法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" rel="tag">动态规划</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8E%9F%E5%9E%8B%E6%A8%A1%E5%BC%8F/" rel="tag">原型模式</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%9B%9E%E6%BA%AF/" rel="tag">回溯</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%9B%A0%E7%89%B9%E7%BD%91%E5%8E%86%E5%8F%B2/" rel="tag">因特网历史</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%9B%BE/" rel="tag">图</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%9B%BE%E7%AE%97%E6%B3%95/" rel="tag">图算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AF%86%E7%A0%81%E5%AD%A6/" rel="tag">密码学</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%B7%A5%E5%8E%82%E6%96%B9%E6%B3%95%E6%A8%A1%E5%BC%8F/" rel="tag">工厂方法模式</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%B9%B6%E6%9F%A5%E9%9B%86/" rel="tag">并查集</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F/" rel="tag">微信小程序</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%84%9F%E7%9F%A5%E6%9C%BA/" rel="tag">感知机</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/" rel="tag">提升方法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/" rel="tag">操作系统</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" rel="tag">支持向量机</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" rel="tag">数据结构</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E8%AE%BA/" rel="tag">数论</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/" rel="tag">最大熵模型</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/" rel="tag">朴素贝叶斯算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/" rel="tag">极大似然估计</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%A8%A1%E6%9D%BF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%BC%8F/" rel="tag">模板方法模式</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%AF%95%E4%B8%9A/" rel="tag">毕业</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%AF%95%E4%B8%9A%E6%97%85%E8%A1%8C/" rel="tag">毕业旅行</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6/" rel="tag">离散数学</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%AD%96%E7%95%A5%E6%A8%A1%E5%BC%8F/" rel="tag">策略模式</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%AE%97%E6%B3%95/" rel="tag">算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" rel="tag">统计学习方法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%A3%85%E9%A5%B0%E6%A8%A1%E5%BC%8F/" rel="tag">装饰模式</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/" rel="tag">计算机网络</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/" rel="tag">设计原则</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/" rel="tag">贝叶斯估计</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95/" rel="tag">软件测试</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%92%E5%BD%92/" rel="tag">递归</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%80%BB%E8%BE%91%E6%96%AF%E8%B0%9B%E5%9B%9E%E5%BD%92/" rel="tag">逻辑斯谛回归</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%95%BF%E6%B2%99/" rel="tag">长沙</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%9D%A2%E7%BB%8F/" rel="tag">面经</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83/" rel="tag">高斯分布</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/AdaBoost/" style="font-size: 10px;">AdaBoost</a> <a href="/tags/DFS/" style="font-size: 10px;">DFS</a> <a href="/tags/EM-%E7%AE%97%E6%B3%95/" style="font-size: 10px;">EM 算法</a> <a href="/tags/UI%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95/" style="font-size: 10px;">UI自动化测试</a> <a href="/tags/k-%E8%BF%91%E9%82%BB/" style="font-size: 10px;">k 近邻</a> <a href="/tags/kd-%E6%A0%91/" style="font-size: 10px;">kd 树</a> <a href="/tags/%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F/" style="font-size: 10px;">代理模式</a> <a href="/tags/%E5%86%B3%E7%AD%96%E6%A0%91/" style="font-size: 10px;">决策树</a> <a href="/tags/%E5%88%86%E6%B2%BB%E7%AE%97%E6%B3%95/" style="font-size: 10px;">分治算法</a> <a href="/tags/%E5%89%8D%E5%90%91%E5%88%86%E6%AD%A5%E6%96%B9%E6%B3%95/" style="font-size: 10px;">前向分步方法</a> <a href="/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" style="font-size: 10px;">动态规划</a> <a href="/tags/%E5%8E%9F%E5%9E%8B%E6%A8%A1%E5%BC%8F/" style="font-size: 10px;">原型模式</a> <a href="/tags/%E5%9B%9E%E6%BA%AF/" style="font-size: 10px;">回溯</a> <a href="/tags/%E5%9B%A0%E7%89%B9%E7%BD%91%E5%8E%86%E5%8F%B2/" style="font-size: 10px;">因特网历史</a> <a href="/tags/%E5%9B%BE/" style="font-size: 10px;">图</a> <a href="/tags/%E5%9B%BE%E7%AE%97%E6%B3%95/" style="font-size: 10px;">图算法</a> <a href="/tags/%E5%AF%86%E7%A0%81%E5%AD%A6/" style="font-size: 10px;">密码学</a> <a href="/tags/%E5%B7%A5%E5%8E%82%E6%96%B9%E6%B3%95%E6%A8%A1%E5%BC%8F/" style="font-size: 10px;">工厂方法模式</a> <a href="/tags/%E5%B9%B6%E6%9F%A5%E9%9B%86/" style="font-size: 10px;">并查集</a> <a href="/tags/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F/" style="font-size: 10px;">微信小程序</a> <a href="/tags/%E6%84%9F%E7%9F%A5%E6%9C%BA/" style="font-size: 10px;">感知机</a> <a href="/tags/%E6%8F%90%E5%8D%87%E6%96%B9%E6%B3%95/" style="font-size: 10px;">提升方法</a> <a href="/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/" style="font-size: 10px;">操作系统</a> <a href="/tags/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" style="font-size: 10px;">支持向量机</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" style="font-size: 10px;">数据结构</a> <a href="/tags/%E6%95%B0%E8%AE%BA/" style="font-size: 10px;">数论</a> <a href="/tags/%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/" style="font-size: 10px;">最大熵模型</a> <a href="/tags/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/" style="font-size: 10px;">朴素贝叶斯算法</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 20px;">机器学习</a> <a href="/tags/%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/" style="font-size: 10px;">极大似然估计</a> <a href="/tags/%E6%A8%A1%E6%9D%BF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%BC%8F/" style="font-size: 10px;">模板方法模式</a> <a href="/tags/%E6%AF%95%E4%B8%9A/" style="font-size: 10px;">毕业</a> <a href="/tags/%E6%AF%95%E4%B8%9A%E6%97%85%E8%A1%8C/" style="font-size: 10px;">毕业旅行</a> <a href="/tags/%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6/" style="font-size: 15px;">离散数学</a> <a href="/tags/%E7%AD%96%E7%95%A5%E6%A8%A1%E5%BC%8F/" style="font-size: 10px;">策略模式</a> <a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 15px;">算法</a> <a href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" style="font-size: 20px;">统计学习方法</a> <a href="/tags/%E8%A3%85%E9%A5%B0%E6%A8%A1%E5%BC%8F/" style="font-size: 10px;">装饰模式</a> <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/" style="font-size: 10px;">计算机网络</a> <a href="/tags/%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/" style="font-size: 10px;">设计原则</a> <a href="/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/" style="font-size: 10px;">贝叶斯估计</a> <a href="/tags/%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95/" style="font-size: 10px;">软件测试</a> <a href="/tags/%E9%80%92%E5%BD%92/" style="font-size: 10px;">递归</a> <a href="/tags/%E9%80%BB%E8%BE%91%E6%96%AF%E8%B0%9B%E5%9B%9E%E5%BD%92/" style="font-size: 10px;">逻辑斯谛回归</a> <a href="/tags/%E9%95%BF%E6%B2%99/" style="font-size: 10px;">长沙</a> <a href="/tags/%E9%9D%A2%E7%BB%8F/" style="font-size: 10px;">面经</a> <a href="/tags/%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83/" style="font-size: 10px;">高斯分布</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/11/">十一月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/06/">六月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/05/">五月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/04/">四月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/03/">三月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/02/">二月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/01/">一月 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/12/">十二月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">五月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">四月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">二月 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2021/11/29/public/webpushr-sw/">(no title)</a>
          </li>
        
          <li>
            <a href="/2021/11/29/public/newPost/">(no title)</a>
          </li>
        
          <li>
            <a href="/2021/06/30/%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B/">《统计学习方法》读书笔记 · 隐马尔可夫模型</a>
          </li>
        
          <li>
            <a href="/2021/06/24/2021%E6%AF%95%E4%B8%9A%E7%A2%8E%E7%A2%8E%E5%BF%B5/">2021毕业碎碎念</a>
          </li>
        
          <li>
            <a href="/2021/06/24/%E6%AF%95%E4%B8%9A%E6%97%85%E8%A1%8C2021/">毕业旅行2021</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2021 xyy<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>