<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="xyy">





<title>《统计学习方法》读书笔记 · 感知机 | xyy爱吃番茄</title>



    <link rel="icon" href="https://qiniu.xuyuyan.cn/XYY.png">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        
            <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


        
    


<meta name="generator" content="Hexo 5.4.0"></head>
<body>
    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">xyy爱吃番茄！</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">文章</a>
                
                    <a class="menu-item" href="/category">分类</a>
                
                    <a class="menu-item" href="/tag">标签</a>
                
                    <a class="menu-item" href="/about">关于我</a>
                
                    <a class="menu-item" href="/friends">友链</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">xyy爱吃番茄！</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">文章</a>
                
                    <a class="menu-item" href="/category">分类</a>
                
                    <a class="menu-item" href="/tag">标签</a>
                
                    <a class="menu-item" href="/about">关于我</a>
                
                    <a class="menu-item" href="/friends">友链</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
        <div class="main">
            <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    document.ready(
        function () {
            tocbot.init({
                tocSelector: '.tocbot-list',
                contentSelector: '.post-content',
                headingSelector: 'h1, h2, h3, h4, h5',
                collapseDepth: 1,
                orderedList: false,
                scrollSmooth: true,
            })
        }
    )

    function expand_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 6,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "collapse_toc()");
        b.innerHTML = "Collapse all"
    }

    function collapse_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 1,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "expand_toc()");
        b.innerHTML = "Expand all"
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">《统计学习方法》读书笔记 · 感知机</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">xyy</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">四月 27, 2021&nbsp;&nbsp;17:46:43</a>
                        </span>
                    
                    
                        <span class="post-category">
                    Category:
                            
                                <a href="/categories/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">《统计学习方法》读书笔记</a>
                            
                        </span>
                    
                </div>
            
        </header>

        <div class="post-content">
            <p>李航老师的《统计学习方法》感知机部分的读书笔记（xyy 要努力做一个日更博主！！）</p>
<span id="more"></span>
<h3 id="0-Intro"><a href="#0-Intro" class="headerlink" title="0. Intro"></a>0. Intro</h3><p>感知机可以理解 <font style="background: #F9EDA6">为将实例划分为正负两类的分离超平面</font>，是二分类的线性分类模型。</p>
<p>用一个很简单的二维的例子来形容就是，在一个分布着红色实例和绿色实例的二维空间里，其中两种颜色的实例线性可分（可以用一条直线将两类实例完全区分开）。在这种情况下，蓝色线（二维空间的超平面是一维的直线）就是感知机。</p>
<img src="https://qiniu.xuyuyan.cn/image-20210427163206496.png" alt="image-20210427163206496" style="zoom:33%;" />

<ul>
<li><p>输入为特征向量，输出为实例的类别（取 +1 和 -1 二值）。</p>
</li>
<li><p>具有简单、易于实现的优点</p>
</li>
<li><p>分为原始形式、对偶形式。</p>
</li>
</ul>
<br>

<h3 id="1-感知机模型简介"><a href="#1-感知机模型简介" class="headerlink" title="1. 感知机模型简介"></a>1. 感知机模型简介</h3><blockquote>
<p><strong>定义（感知机）</strong>：假设输入空间是 $\chi \in R^n$，输出空间是 $y = {+1, -1}$。输入 $x \in \chi$ 表示实例的特征向量，对应于输入空间。</p>
<p>则感知机的由输入空间到输出空间的映射函数为：$f(x) = sign(w*x+b)$</p>
<p>其中，$w$ 和 $b$ 为感知机参数，$w \in R^n$ 叫做权值向量，$b \in R$ 叫做偏置。</p>
<p>$sign$ 为符号函数，即：$sign = \begin{cases} +1 &amp;\text{x &gt;=0} \\-1 &amp;\text{x&lt;0} \end{cases}$</p>
</blockquote>
<p><font style="background: #d4e9d6"><strong>感知机的几何解释：</strong></font></p>
<p>线性方程 $w*x+b=0$ 对应于特征空间 $R^n$ 中的一个超平面 S，其中 <font style="background: #F9EDA6">w 是超平面的法向量，b 是超平面的截距</font>。</p>
<br>

<h3 id="2-感知机的学习策略"><a href="#2-感知机的学习策略" class="headerlink" title="2. 感知机的学习策略"></a>2. 感知机的学习策略</h3><h4 id="2-1-线性可分介绍"><a href="#2-1-线性可分介绍" class="headerlink" title="2.1 线性可分介绍"></a>2.1 线性可分介绍</h4><blockquote>
<p>定义（数据集的线性可分性）：给定一个数据集 $T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$，其中 $x_i \in R^n$，$y_i \in {+1, -1}$，$i=1,2…,N$。如果存在某个超平面 $S$ 能将数据集的正实例点和负实例点完全正确地划分到超平面地两侧，则称数据集具有线性可分性。（即对所有 $y_i = +1$ 的实例 $i$，有 $wx_i+b&gt;0$；对所有 $y_i=-1$ 的实例 $i$，有 $wx_i+b&lt;0$）</p>
</blockquote>
<p><strong>假设训练数据集是线性可分的</strong>，感知机学习的目标是求得一个能够将训练集正实例点和负实例点完全正确分开的分离超平面。为此需要确定一个学习策略（损失函数）来确定优秀的感知机模型参数 $w$ 和 $b$。<br>`</p>
<h4 id="2-2-感知机的损失函数推导："><a href="#2-2-感知机的损失函数推导：" class="headerlink" title="2.2 感知机的损失函数推导："></a>2.2 感知机的损失函数推导：</h4><p>感知机采用的损失函数原理为： <font style="background: #F9EDA6">误分类点到超平面 $S$ 的总距离</font>。</p>
<ol>
<li><p>先写出输入空间 $R^n$ 中任一点 $x_0$ 到超平面 $S$ 的距离：$\frac{1}{||w||} |w*x_0+b|$ （此处 $||w||$ 为 $w$ 的 $L_2$ 范数）</p>
</li>
<li><p>其次，对于误分类的点来说，$-y_i(w*x+b)&gt;=0$</p>
</li>
<li><p>所以，误分类点 $x_i$ 到超平面 $S$ 的距离是：$-\frac{1}{||w||}y_i(w*x_i+b)$</p>
</li>
<li><p>因此，感知机的损失函数为所有误分类点到超平面 $S$ 的总距离：$-\frac{1}{||w||}\sum_{x_i \in M} y_i(w*x_i+b)$</p>
</li>
</ol>
<p>不考虑 $\frac{1}{||w||}$，结果为 $L(w,b)=-\sum_{x_i \in M}y_i(w*x_i+b)$</p>
<blockquote>
<p><font style="background: #d4e9d6"><strong>补充介绍 <code>函数间距</code> 和 <code>几何间距</code> ：</strong></font></p>
<p>可能大家对 $|w*x_0+b|$ （函数间距）比较熟悉，但是函数间距存在一定的问题。我们引入损失函数的目的是唯一地衡量超平面的划分性能，因此，固定的超平面应该拥有唯一的 Loss。但是聪明的感知机发现，如果等比例地缩小 $w$ 和 $b$，可以轻而易举地降低 Loss。但是，超平面却没有改变！</p>
<p>因此引入了$\frac{1}{||w||} |w*x_0+b|$（几何间距），拥有等比例的感知机参数将拥有相同的 Loss</p>
</blockquote>
<br>

<h3 id="3-感知机学习算法（原始形式）"><a href="#3-感知机学习算法（原始形式）" class="headerlink" title="3. 感知机学习算法（原始形式）"></a>3. 感知机学习算法（原始形式）</h3><p>原始形式的感知机学习过程可以分为三步：</p>
<ol>
<li><p> <font style="background: #F9EDA6">随机选取感知机模型参数初值 $w_0$，$b_0$</font></p>
</li>
<li><p> <font style="background: #F9EDA6">采用梯度下降法极小化目标函数</font></p>
</li>
</ol>
<p>   $L(w,b)=-\sum_{x_i \in M}y_i(w*x_i+b)$</p>
<p>   $\nabla_w L(w,b)=-\sum_{x_i \in M} y_ix_i$</p>
<p>   $\nabla_b L(w,b)=-\sum_{x_i \in M} y_i$</p>
<ol start="3">
<li> <font style="background: #F9EDA6">更新 $w$，$b$ </font>（此处 $\eta$ 为步长，也称学习率）</li>
</ol>
<p>   $w \leftarrow w + \eta y_i x_i$</p>
<p>   $b \leftarrow b + \eta y_i$</p>
<br>

<h3 id="4-感知机学习算法（对偶形式）"><a href="#4-感知机学习算法（对偶形式）" class="headerlink" title="4. 感知机学习算法（对偶形式）"></a>4. 感知机学习算法（对偶形式）</h3><h4 id="4-1-对偶形式介绍"><a href="#4-1-对偶形式介绍" class="headerlink" title="4.1 对偶形式介绍"></a>4.1 对偶形式介绍</h4><p>我们知道在原始形式中，感知机模型参数 $w$ 和 $b$ 的每一次迭代为：$w \leftarrow w + \eta y_i x_i$ 和 $b \leftarrow b + \eta y_i$。</p>
<p>那我们<font style="background: #F9EDA6">整合 n 次迭代，可以得到 $w$ 和 $b$ 关于 $(x_i, y_i)$ 的增量分别是 $\alpha_i y_i x_i$ 和 $\alpha_i y_i$ </font>（为了简洁 $\alpha$ 在这里视作一个新的常量，为 $n_i \eta$）</p>
<p>所以最后学习到的就是：<font style="background: #F9EDA6">$w = \sum_{i=1}^{N} \alpha_i y_i x_i$</font> 和 <font style="background: #F9EDA6">$b = \sum_{i=1}^{N} \alpha_i y_i$ </font></p>
<p>然后将上式<strong>代入</strong>感知机的输入空间至输出空间的映射关系 $f(x)=sign(w*x+b)$ 得到 <font style="background: #F9EDA6">$f(x)=sign(\sum_{j=1}^{N} \alpha_jy_jx_j * x + b)$</font></p>
<p>至此我们得到了感知机学习的对偶模式的学习算法。</p>
<h4 id="4-2-对偶模式学习步骤"><a href="#4-2-对偶模式学习步骤" class="headerlink" title="4.2 对偶模式学习步骤"></a>4.2 对偶模式学习步骤</h4><p>对偶模式学习步骤总体和传统模型相近：</p>
<ol>
<li><p>选取感知机模型参数初值 <font style="background: #FBD4D0">$\alpha_0 = 0$，$b_0 = 0$ </font></p>
<p><font style="background: #FBD4D0">【注意对比传统模式】</font>传统模式的参数初值是随机选取。这里因为 $\alpha$ 具有涵义 $n_i \eta$，而初始情况下是不知道有无误分类点，因此取 0</p>
</li>
<li><p>在训练集选取数据 $(x_i, y_i)$</p>
</li>
<li><p>如果（误分类）$y_i (\sum_{j=1}^{N} \alpha_j y_j x_j* x_i + b) \leq 0$</p>
<p>更新 $w$，$b$ ：$\alpha_i \leftarrow \alpha_i + \eta$ 和 $b \leftarrow b + \eta y_i$</p>
</li>
<li><p>转至 2 直到没有误分类点 </p>
</li>
</ol>
<h4 id="4-3-WHY-对偶模式？"><a href="#4-3-WHY-对偶模式？" class="headerlink" title="4.3 WHY 对偶模式？"></a>4.3 WHY 对偶模式？</h4><p>我们先将两种模式的参数更新表达式单独列出：</p>
<table>
<thead>
<tr>
<th>模式</th>
<th>参数1</th>
<th>参数2</th>
<th>映射关系</th>
</tr>
</thead>
<tbody><tr>
<td>传统模式</td>
<td>$w \leftarrow w + \eta y_i x_i$</td>
<td>$b \leftarrow b + \eta y_i$</td>
<td>$f(x)=sign(w*x+b)$</td>
</tr>
<tr>
<td>对偶模式</td>
<td>$\alpha_i \leftarrow \alpha_i + \eta$</td>
<td>$b \leftarrow b + \eta y_i$</td>
<td>$f(x)=sign(\sum_{j=1}^{N} \alpha_jy_jx_j * x + b)$</td>
</tr>
</tbody></table>
<p> 我们可以比较容易地看出<font style="background: #F9EDA6">在传统模式下，每次迭代参数 $w$ 都需计算一遍内积 $y_i x_i$，在计算映射关系时同样也需要计算内积 $w*x$</font></p>
<p>但是在对偶模式下，<strong>参数的迭代只是简单的加法运算</strong>，映射关系虽然需要计算内积 $x_j * x_i$ ，但是我们利用 <code>Gram矩阵</code> 可以较好地解决。因此<strong>大大减少了计算量</strong>。</p>
<p><font style="background: #FBD4D0">【Gram矩阵】</font>定义式为：$G = [x_i * x_j]_{N*N}$，在此处的用途是，可以<font style="background: #FBD4D0">将一系列的内积 $x_i * x_j$ 提前计算好，在执行映射关系计算的时候，仅需要查表，将时间复杂度降到 O(1) </font></p>
<br>

<h3 id="5-算法的收敛性"><a href="#5-算法的收敛性" class="headerlink" title="5. 算法的收敛性"></a>5. 算法的收敛性</h3><p>为了便于叙述和推导，将偏置 $b$ 并入权重向量 $w$，记作 $\hat w = (w^T,b)^T$ </font>。同样也将输入向量加以扩充，加入常数1，记作 <font style="background: #F9EDA6">$\hat x = (x^T, 1)^T$</font></p>
<p>我们<font style="background: #d4e9d6">讨论收敛性的目的</font>是为了证明<strong>在训练集线性可分的情况下</strong>，误分类次数 $k$ 是由上限的。换句话说就是<strong>经过有限次搜索</strong>，可以找到<strong>将训练集完全正确分开</strong>的分离超平面。当训练集线性不可分的时候，感知机学习算法不收敛，迭代结果会发生震荡。</p>
<h4 id="5-1-两个不等式证明"><a href="#5-1-两个不等式证明" class="headerlink" title="5.1 两个不等式证明"></a>5.1 两个不等式证明</h4><ul>
<li><p><font style="background: #F9EDA6">不等式1：$\hat w_k * \hat w_{opt} \geq k \eta \gamma$</font></p>
<p>$\hat w_k * \hat w_{opt} = \hat w_{k-1} * \hat w_{opt} + \eta y_i \hat w_{opt} * \hat x_i \geq \hat w_{k-1} * \hat w_{opt} + \eta \gamma$</p>
<p>由此递推可得 $\hat w_k * \hat w_{opt} \geq \hat w_{k-1}* \hat w_{opt} + \eta \gamma \geq \hat w_{k-2}* \hat w_{opt} + 2\eta \gamma \geq … \geq k \eta \gamma$</p>
</li>
<li><p><font style="background: #F9EDA6">不等式2：$||\hat w_k||^2 \leq k \eta^2R^2$</font></p>
<p>$||\hat w_k||^2 = ||\hat w_{k-1}||^2 + 2\eta y_i \hat w_{k-1} * \hat x_i + \eta^2||\hat x_i||^2$</p>
<p>$\leq ||\hat w_{k-1}||^2 + \eta^2||\hat x_i||^2$</p>
<p>$\leq ||\hat w_{k-1}||^2 + \eta^2R^2$</p>
<p>$\leq ||\hat w_{k-2}||^2 + 2\eta^2R^2$</p>
<p>$\leq k \eta^2R^2$</p>
</li>
</ul>
<h4 id="5-2-Novikoff-定理"><a href="#5-2-Novikoff-定理" class="headerlink" title="5.2 Novikoff 定理"></a>5.2 Novikoff 定理</h4><blockquote>
<p><strong>定理（Novikoff）：</strong>设训练数据集 $T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$ 是线性可分的，其中 $x_i \in \chi = R^n$，$y_i \in {+1, -1}$，$i=1,2,…,N$，则：</p>
<ol>
<li>存在满足条件 $||\hat w_{opt}||=1$ 的超平面 $\hat w_{opt} \hat x = w_{opt} x + b_{opt} = 0$ 将训练数据集完全分开；<br>  且存在 $\gamma &gt; 0$，对所有 $i=1,2,…,N$ 有 $y_i(\hat w_{opt} \hat x_i) = y_i (w_{opt}  x_i + b_{opt}) &gt;= \gamma$</li>
<li>令 $R = max_{1 \leq i \leq N} ||\hat x_i||$，则感知机算法在训练数据集上的误分类次数 $k$ 满足不等式 $k \leq (\frac{R}{\gamma})^2$</li>
</ol>
</blockquote>
<p>Novikoff 定理主要<font style="background: #F9EDA6">为感知机在理论上的可行性做了证明</font>，误分类次数具有优先上限，意味着在训练集线性可分的情况下，感知机通过有限次迭代参数，最终可以完全分类。</p>
<p>接下来进行证明：</p>
<ol>
<li><p>由于假定了前提条件训练集是线性可分的</p>
<p>因此，设将训练数据集完全正确分开的超平面为 $\hat w_{opt}  \hat x = w_{opt}  x + b_{opt} = 0$，使 $||\hat w_{opt}|| = 1$</p>
<p>由于对 $i = 1,2,..,N$ 均有 $y_i (\hat w_{opt}  \hat x_i) = y_i (w_{opt}  x_i + b_{opt}) &gt; 0$</p>
<p>所以必定存在 $\gamma = min_i {y_i (w_{opt}  x_i + b_{opt})}$ 使 $y_i (\hat w_{opt}  \hat x_i) = y_i(w_{opt}  x_i + b_{opt}) \geq \gamma$</p>
</li>
<li><p>令 $\hat w_{k-1}$ 是第 $k$ 个误分类实例之前的扩充权重向量，即 $\hat w_{k-1} = (w_{k-1}^{T}, b_{k-1})^T$</p>
<p>则第 $k$ 个误分类实例的条件是 $y_i (\hat w_{k-1}  \hat x_i) = y_i (w_{k-1}  x_i + b_{k-1}) \leq 0$</p>
<p>$w$ 和 $b$ 的更新为 $w_k \leftarrow w_{k-1} + \eta y_i x_i$ 和 $b_k \leftarrow b_{k-1}+\eta y_i$</p>
<p>借助上节推导的两个不等式可得：</p>
<p>$k \eta \gamma \leq \hat w_k * \hat w_{opt} \leq ||\hat w_k||||\hat w_{opt}|| \leq \sqrt{k}\eta R$</p>
<p>等价于$k^2 \gamma^2 \leq kR^2$</p>
<p>所以可得 $k \leq (\frac{R}{\gamma})^2$</p>
</li>
</ol>
<br>

<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>感知机通过<font style="background: #F9EDA6">构造超平面</font>对实例点进行分类（二分类），其中构造的超平面依赖于感知机参数 $w$ 和 $b$，两者在每一个误分类点处使用梯度下降的方法进行迭代更新。</p>
<p>但是，感知机起作用有一个<font style="background: #F9EDA6">很强的前提</font>：训练集是线性可分的（或者说，判别边界是线性的），只有在这个前提下才能训练出完全划分的感知机，不然在迭代参数的时候会震荡。</p>

        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>xyy</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>Permalink:</span>
                        <span><a href="http://www.xuyuyan.cn/2021/04/27/%E6%84%9F%E7%9F%A5%E6%9C%BA/">http://www.xuyuyan.cn/2021/04/27/%E6%84%9F%E7%9F%A5%E6%9C%BA/</a></span>
                    </p>
                
                
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"># 机器学习</a>
                    
                        <a href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"># 统计学习方法</a>
                    
                        <a href="/tags/%E6%84%9F%E7%9F%A5%E6%9C%BA/"># 感知机</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2021/04/29/%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/">《统计学习方法》读书笔记 · k 近邻算法</a>
            
            
            <a class="next" rel="next" href="/2021/03/16/%E7%94%B5%E9%AD%82%E4%B8%80%E9%9D%A2/">电魂一面</a>
            
        </section>


    </article>
</div>


    <div id="gitalk-container"></div>
    <link rel="stylesheet" href="//unpkg.com/gitalk/dist/gitalk.css">
<script src="//unpkg.com/gitalk/dist/gitalk.min.js"></script>
<script src="//cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.js"></script>
<div id="gitalk-container"></div>
<script type="text/javascript">
      var gitalk = new Gitalk({
        clientID: '4283629b368b1cf98f81',
        clientSecret: '8bb6ce27010c0db8286e943f3765aeae1757de4d',
        repo: 'xyyBlog',
        owner: 'rdzfv',
        admin: 'rdzfv',
        id: md5(location.pathname),      
        labels: 'Gitalk'.split(',').filter(l => l),
        perPage: 10,
        pagerDirection: 'last',
        createIssueManually: true,
        distractionFreeMode: true
      })
      gitalk.render('gitalk-container')
</script>



        </div>
        <footer id="footer" class="footer">
   <div class="copyright">
    <div style="position:fixed;left:8%;bottom:5%;">
        <div id="webpushr-subscription-button" data-button-text="订阅本站" data-subscriber-count-text="人已订阅" data-background-color="#000000"></div>
    </div>
    <div style="display:inline-block;">
        <div> <a target="_blank" rel="noopener" href="http://www.beian.miit.gov.cn/" style="text-decoration: none;color: black;" >浙ICP备19012712号</a> </div>
        <span>© xyy | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span>
    </div>
</footer>

    </div>
</body>
</html>
